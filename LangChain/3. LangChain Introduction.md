#langchain 

# What is LangChain?
---
LangChain is an open source Python library designed to simplify and enhance the process of building LLM powered applications. 

The `ChatOpenAI` class from `langchain_openai` library is used to define a model we can use in LangChain applications: it makes a request to the OpenAI API, and returns the response back to the application. We use `.invoke()` to prompt the model.

```python
from langchain_openai impport ChatOpenAI

llm = ChatOpenAI(
	model="gpt-4o-mini",
	api_key='...',
	max_completion_tokens=100,
	temperature=0.1
)

# prompt the model using invoke()
llm.invoke("What is LangChain?")
```

# Prompt Templates
---
Prompt Templates allow us to prompt LLMs in a modular and reusable way - they are recipes for defining LLM prompts. They can contain instructions, examples and additional context. They are created using the `PromptTemplate` class:

```python
from langchain_core.prompts import PromptTemplate

# Example template string, {} is for dynamic insertion later in the code.
template = "Explain this concept simply and concisely: {concept}"

# convert the above string to PromptTemplate
prompt_template = PromptTemplate.from_template(template=template)

# insert variable using invoke(), and pass it a dictionary
prompt = prompt_template.invoke({"concept":"General relativity"})
```

# Integrating the Prompt Template and LLM Model
---
To integrate Prompt Template and LLM model, we will use the *LangChain Expression Language (LCEL)*. The `|` operator creates a *Chain*, which is another fundamental LangChain component. In the below example, the user input will be passed to the LLM

![[../images/Pasted image 20260222185719.png]]



```python
# create llm
from langchain_openai impport ChatOpenAI
from langchain_core.prompts import PromptTemplate

llm = ChatOpenAI(
	model="gpt-4o-mini",
	api_key='...',
	max_completion_tokens=100,
	temperature=0.1
)

# create prompt_template
template = "Explain this concept simply and concisely: {concept}"
prompt_template = PromptTemplate.from_template(template=template)

# prompt the model
llm_chain = prompt_template | llm
concept = "General Relativity"
print(llm_chain.invoke({"concept": concept}))
```


# Chat Models
---
Chat Models supports prompting with roles, `system, human, ai`, allowing us to to specify a series of messages from these roles.
Below is how we can create a prompt template including chat message roles, by using the `ChatPromptTemplate` class: it allows us to specify a *list of tuples* containing messages to pass to the chat model.

```python
from langchain_core.prompts import ChatPromptTemplate

template = ChatPromptTemplate.from_messages([
	("system", "You are a calculator that responds with math"),
	("human","Answer: what is two plus two?").
	("ai","2+2=4"),
	("human","Answer this math question: {math}")
])
```

- *System* role is used to describe the model behavior
- *Human* role is to provide user inputs
- *AI* role is used for defining model responses, often used to provide examples the model can learn from.


```python
from langchain_core.prompts import ChatPromptTemplate

template = ChatPromptTemplate.from_messages([
	("system", "You are a calculator that responds with math"),
	("human","Answer: what is two plus two?").
	("ai","2+2=4"),
	("human","Answer this math question: {math}")
])

llm = ChatOpenAI(model="gpt-4o-mini",api_key='OPENAI_API_KEY')
llm_chain = template | llm
math = "What is five plus five?"

response = llm_chain.invoke({"math": math})
print(response.content)
```

# Few-Shot Prompting
---

`PromptTempalte` and `ChatPromptTemplate` are good for handling small number of examples. They *do not scale well* for many examples.

The `FewShotPromptTemplate` class allows us to use data sets like a list of dictionaries with questions and answers (as shown below) into a prompt template, to provide more context to the model.

```python
examples = [
	{
		"question": "What is the capital of France?",
		"answer": "Paris"
	},
	...
]
```

If we have a *Pandas DataFrame*, we can convert it to a list of dictionaries:

```text
||Name|Age|
|0|Alice|25|
|1|Bob|30|
```

`df.to_dict(orient='records')` will create a list of dictionaries, where, *each dictionary is one row*, as shown below:

```python
[{'Name': 'Alice', 'Age': 25}, {'Name': 'Bob', 'Age': 30}]
```

We first need to decide how we want to structure the examples for the LLM:

```python
from langchain.prompts import FewShotPromptTemplate, PromptTemplate

example_prompt = PromptTemplate.from_template("Question": {question}\n{answer})
```

`FewShotPromptTemplate` takes the examples list of dictionaries, and the template for formatting the examples:

```python
prompt_tempalte = FewShotPromptTemplate(
	examples=examples,
	example_prompt=example_prompt,
	suffix="Question: {input}",
	input_variables=["input"]
)
```

`suffix` provides provide the **final question** the LLM needs to answer _after_ it has seen all your examples.
`input_variables`  a list of the **variable names** that the overall `FewShotPromptTemplate` expects you to provide when you eventually call it. It tells LangChain keep an eye out for a variable named 'input' - when the user gives it to us, put it wherever you see `{input}` in the suffix."

```python
examples = [{"word": "happy", "emoji": "ðŸ˜Š"}]
example_prompt = PromptTemplate.from_template(template="Word: {word}\nEmoji: {emoji}")

# THE TEMPLATE
prompt_template = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="Translate words to emojis:",
    suffix="Word: {input}\nEmoji:",  # <--- SUFFIX: The final "blank" to fill
    input_variables=["input"]        # <--- INPUT_VARIABLES: Tells us what key to use
)

# USING IT
print(prompt_template.format(input="dog"))
```

The final input to the LLM would look like this:
Translate words to emojis:

Word: happy Emoji: ðŸ˜Š

**Word: dog** _(This came from the suffix + your input)_ 
**Emoji:** _(This is the end of the suffix, waiting for the AI to type 'ðŸ¶')_

Below is a fully functioning example:

```python
#Â Step 1: CreateÂ theÂ examplesÂ listÂ ofÂ dicts

examplesÂ =Â [
Â Â {
Â Â Â Â "question":Â "HowÂ manyÂ DataCampÂ coursesÂ hasÂ JackÂ completed?",
Â Â Â Â "answer":Â "36"
Â Â },
Â Â {
Â Â Â Â "question":Â "HowÂ muchÂ XPÂ doesÂ JackÂ haveÂ onÂ DataCamp?",
Â Â Â Â "answer":Â "284,320XP"
Â Â },
Â Â {
Â Â Â Â "question":Â "WhatÂ technologyÂ doesÂ JackÂ learnÂ aboutÂ mostÂ onÂ DataCamp?",
Â Â Â Â "answer":Â "Python"
Â Â }
]

# Step 2: Create Few-Shot Prompt Template
example_promptÂ =Â PromptTemplate.from_template("Question:Â {question}\n{answer}")

#Â CreateÂ theÂ few-shotÂ prompt
prompt_templateÂ =Â FewShotPromptTemplate(
Â Â Â Â examples=examples,
Â Â Â Â example_prompt=example_prompt,
Â Â Â Â suffix="Question:Â {input}",
Â Â Â Â input_variables=["input"],
)

# Invoking the prompt template allows you to see exactly what context the model will have
promptÂ =Â prompt_template.invoke({"input":Â "WhatÂ isÂ Jack'sÂ favoriteÂ technologyÂ onÂ DataCamp?"})
print(prompt.text)

# Step 3: create an LCEL chain to combine the few-shot template with an LLM!
llmÂ =Â ChatOpenAI(model="gpt-4o-mini",Â api_key='<OPENAI_API_TOKEN>')
#Â CreateÂ andÂ invokeÂ theÂ chain
llm_chainÂ =Â prompt_templateÂ |Â llm
print(llm_chain.invoke({"input":Â "WhatÂ isÂ Jack'sÂ favoriteÂ technologyÂ onÂ DataCamp?"}))
```

